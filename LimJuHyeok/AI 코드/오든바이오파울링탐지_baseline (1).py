# -*- coding: utf-8 -*-
"""오든바이오파울링탐지_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rCK2oy5s4FfgO6LMqC8k9IflMZegqsqK

# 데이터 임포트
"""

! mkdir data
! unzip /content/drive/MyDrive/projects/oden_anomalydetection/wando_sample.zip -d data

import pandas as pd
import ast

df1 = pd.read_excel("data/wando1_oxygen(9.15~10.14).xlsx")
df2 = pd.read_excel("data/wando2_oxygen(8.15~9.15).xlsx")
df3 = pd.read_excel("data/wando2_oxygen(9.15~10.14).xlsx")

df = pd.concat([df1, df2, df3]).reset_index(drop=True)

df.isnull().sum()

df.duplicated().sum()

"""# 데이터 전처리 및 EDA"""

import matplotlib.pyplot as plt

df.columns

df = df[['device_id', 'date_time', 'temperature', 'oxygen_ppm']]

df['date_time'] = pd.to_datetime(df['date_time'])

col_li = ['temperature', 'oxygen_ppm']

for col in col_li:
  df[col] = df[col].apply(ast.literal_eval)
  df[col] = df[col].apply(lambda x: x['value'])

df.dtypes

pd.to_datetime(df['date_time'])

abnormal_df = df[df['device_id']=="wando01"].reset_index(drop=True)
normal_df = df[df['device_id']=="wando02"].reset_index(drop=True)

abnormal_df.set_index('date_time', inplace=True)
abnormal_df['temperature'] = abnormal_df['temperature'].astype(float)
abnormal_df['oxygen_ppm'] = abnormal_df['oxygen_ppm'].astype(float)

# 2분 단위로 리샘플링 (평균값을 계산)
abnormal_df = abnormal_df.resample('2min').ffill().reset_index()
abnormal_df = abnormal_df.dropna()

normal_df.set_index('date_time', inplace=True)
normal_df['temperature'] = normal_df['temperature'].astype(float)
normal_df['oxygen_ppm'] = normal_df['oxygen_ppm'].astype(float)

# 2분 단위로 리샘플링 (평균값을 계산)
normal_df = normal_df.resample('2min').ffill().reset_index()
normal_df = normal_df.dropna()

print(normal_df['oxygen_ppm'].quantile(0.001))
print(normal_df['oxygen_ppm'].quantile(0.01))
print(normal_df['oxygen_ppm'].quantile(0.025))
print(normal_df['oxygen_ppm'].quantile(0.05))
print(normal_df['oxygen_ppm'].quantile(0.1))
print(normal_df['oxygen_ppm'].quantile(0.2))
print(normal_df['oxygen_ppm'].quantile(0.3))

print(abnormal_df['oxygen_ppm'].quantile(0.1))
print(abnormal_df['oxygen_ppm'].quantile(0.125))
print(abnormal_df['oxygen_ppm'].quantile(0.15))
print(abnormal_df['oxygen_ppm'].quantile(0.175))
print(abnormal_df['oxygen_ppm'].quantile(0.2))
print(abnormal_df['oxygen_ppm'].quantile(0.3))
print(abnormal_df['oxygen_ppm'].quantile(0.7))
print(abnormal_df['oxygen_ppm'].quantile(0.9))
print(abnormal_df['oxygen_ppm'].quantile(0.95))

normal_df.describe()

abnormal_df.describe()

# 그래프 그리기
target = 'oxygen_ppm'
plt.plot(normal_df['date_time'], normal_df[target], marker='.', color='green', markersize=2)
plt.plot(abnormal_df['date_time'], abnormal_df[target], marker='.', color='red', markersize=2)
plt.title(f'{target} over Time')
plt.xlabel('Date')
plt.ylabel('Value')
plt.grid(True)
plt.xticks(rotation=45)  # x축 라벨 회전
plt.tight_layout()  # 레이아웃 조정
plt.show()

"""oxygen ppm이 3이하가 지속될 경우 용존산소 센서 이상일 가능성이 매우 높다.

그리고 센서 이상이 생길 시 용존산소에 급격한 변화를 보이는 것으로 추정되는데,

wando2(normal_df)의 경우에도 10월 10일 이후 센서의 이상이 확인된다.
"""

#normal_df2 = normal_df[normal_df['date_time']<='2024-10-13']
normal_df2 = normal_df.copy()

normal_df2

"""# 학습용 데이터 구축"""

import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

# Parameters
sequence_length = 30  # 1 hour sequences (30 time steps of 2 minutes)
features = ['oxygen_ppm']

# Function to create sequences from the data
def create_sequences(df, label):
    df = df.sort_values('date_time').reset_index(drop=True)
    data = df[features].values
    sequences = []
    labels = []
    num_sequences = len(data) - sequence_length + 1
    for i in range(num_sequences):
        seq = data[i:i+sequence_length]
        sequences.append(seq)
        labels.append(label)
    return sequences, labels

# Create sequences for normal data (label=0)
normal_sequences, normal_labels = create_sequences(normal_df2, label=0)

# Create sequences for abnormal data (label=1)
abnormal_sequences, abnormal_labels = create_sequences(abnormal_df, label=1)

# Combine normal and abnormal data
all_sequences = np.concatenate([normal_sequences, abnormal_sequences], axis=0)
all_labels = np.concatenate([normal_labels, abnormal_labels], axis=0)

# Shuffle the data
indices = np.arange(len(all_sequences))
np.random.shuffle(indices)
all_sequences = all_sequences[indices]
all_labels = all_labels[indices]

# Define PyTorch Dataset
class OxygenDataset(Dataset):
    def __init__(self, sequences, labels):
        self.sequences = torch.tensor(sequences, dtype=torch.float32)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx]

# Create the dataset
dataset = OxygenDataset(all_sequences, all_labels)

# Example usage with DataLoader
batch_size = 64
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

from torch.utils.data import random_split

# Parameters for train/validation/test split
train_ratio = 0.7
val_ratio = 0.15
test_ratio = 0.15

# Calculate lengths for each split
dataset_size = len(dataset)
train_size = int(train_ratio * dataset_size)
val_size = int(val_ratio * dataset_size)
test_size = dataset_size - train_size - val_size  # Ensures no leftover samples

# Split the dataset
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# Create DataLoaders for each split
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Example usage with train DataLoader
for batch_sequences, batch_labels in train_dataloader:
    # batch_sequences shape: [batch_size, sequence_length, num_features]
    # batch_labels shape: [batch_size]
    # Here you would pass batch_sequences to your GRU model for training
    pass  # Replace with your training code

print(len(train_dataset), len(val_dataset), len(test_dataset))

"""# AI 모델 학습"""

! mkdir checkpoint

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
import os
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import numpy as np

# Define GRU-based model
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.gru(x)  # out: [batch_size, seq_len, hidden_size]
        out = self.fc(out[:, -1, :])  # Use the last time step's output
        return out

# Model parameters
input_size = len(features)  # Number of features (e.g., temperature, oxygen_ppm)
hidden_size = 32
output_size = 2  # Binary classification (normal/abnormal)
num_layers = 1
epochs = 20
learning_rate = 0.001
best_val_loss = float('inf')
best_model_path = "checkpoint/best_gru_model.pth"

# Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Instantiate the model, define loss function and optimizer
model = GRUModel(input_size, hidden_size, output_size, num_layers).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Learning rate scheduler to reduce learning rate if validation loss does not improve
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

# Early stopping parameters
early_stop_patience = 5
no_improvement_count = 0

# Lists to store loss values for plotting
train_losses = []
val_losses = []

# Training loop
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    # Add tqdm for training progress visualization
    train_loader_tqdm = tqdm(train_dataloader, desc=f"Epoch [{epoch+1}/{epochs}] Training")
    for batch_sequences, batch_labels in train_loader_tqdm:
        batch_sequences, batch_labels = batch_sequences.to(device), batch_labels.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(batch_sequences)
        loss = criterion(outputs, batch_labels)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        # Track loss and accuracy
        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct_predictions += (predicted == batch_labels).sum().item()
        total_predictions += batch_labels.size(0)

    # Calculate training loss and accuracy
    epoch_loss = running_loss / len(train_dataloader)
    epoch_accuracy = correct_predictions / total_predictions
    train_losses.append(epoch_loss)
    print(f"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}")

    # Validation loop
    model.eval()
    val_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    with torch.no_grad():
        val_loader_tqdm = tqdm(val_dataloader, desc=f"Epoch [{epoch+1}/{epochs}] Validation")
        for batch_sequences, batch_labels in val_loader_tqdm:
            batch_sequences, batch_labels = batch_sequences.to(device), batch_labels.to(device)
            outputs = model(batch_sequences)
            loss = criterion(outputs, batch_labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == batch_labels).sum().item()
            total_predictions += batch_labels.size(0)

    # Calculate validation loss and accuracy
    val_epoch_loss = val_loss / len(val_dataloader)
    val_epoch_accuracy = correct_predictions / total_predictions
    val_losses.append(val_epoch_loss)
    print(f"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}")

    # Save the best model based on validation loss
    if val_epoch_loss < best_val_loss:
        best_val_loss = val_epoch_loss
        torch.save(model.state_dict(), best_model_path)
        print(f"Best model saved with validation loss: {best_val_loss:.4f}")
        print("===============================================================================================")
        no_improvement_count = 0  # Reset early stopping counter
    else:
        no_improvement_count += 1

    # Reduce learning rate if validation loss does not improve
    scheduler.step(val_epoch_loss)

    # Early stopping check
    if no_improvement_count >= early_stop_patience:
        print(f"Early stopping triggered after {epoch+1} epochs.")
        break

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')
plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss Over Epochs')
plt.legend()
plt.show()

"""# 검증"""

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# Testing loop
all_labels = []
all_predictions = []
test_loss = 0.0
correct_predictions = 0
total_predictions = 0
model.eval()

# Load the best model for testing
if os.path.exists(best_model_path):
    model.load_state_dict(torch.load(best_model_path))
    print("Loaded the best model for testing.")

with torch.no_grad():
    test_loader_tqdm = tqdm(test_dataloader, desc="Testing")
    for batch_sequences, batch_labels in test_loader_tqdm:
        batch_sequences, batch_labels = batch_sequences.to(device), batch_labels.to(device)
        outputs = model(batch_sequences)
        loss = criterion(outputs, batch_labels)
        test_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct_predictions += (predicted == batch_labels).sum().item()
        total_predictions += batch_labels.size(0)
        all_labels.extend(batch_labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Calculate test loss and accuracy
test_epoch_loss = test_loss / len(test_dataloader)
test_epoch_accuracy = correct_predictions / total_predictions
print(f"Test Loss: {test_epoch_loss:.4f}, Test Accuracy: {test_epoch_accuracy:.4f}")

# Confusion Matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Abnormal'], yticklabels=['Normal', 'Abnormal'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
class_report = classification_report(all_labels, all_predictions, target_names=['Normal', 'Abnormal'])
print("Classification Report:\n", class_report)



"""든생각 :
hidden_size
num_layers
sequence_length
learning_rate
epochs

등 AI 모델의 성능에 영향을 줄 수 있는 변수들이 많은데,
내가 한 선택이 최적이라는 것을 증명하기 위한 근거가 뒷받침 되면 좋을 것 같다.

2. 추가적인 바이오파울링 발생 데이터 확보 필요 : 평가에 신뢰도 상승

연구적으로 가치가 있으려면
중간 부분의 데이터가 있어야함
0으로 깔리는 데이터

온도는 같이 들어가면 정확도가 높아질 수 밖에 없는게

따개비가 붙은 다른 데이터들을 받아봐야함

시간 슬라이딩/셔플 -> 유지


#과적합
Train loss -

Validation loss

Loss가 올라가는 구간 파악


1. 추가 데이터
 - 새로 생긴 wando01 기기, 최신 데이터 (10-14 ~ 10-22)
 - 논문 최종 버전 나오기 전에 한번 최신화

2. 실험 알고리즘
 - RNN
 - GRU
 - LSTM
 - Time Distribute : Time series custom Dense(1차원 뉴런) : 가장 기초적
 - time series custom CNN (도전)
시퀀스적인걸 고려

3. 지표 (알고리즘 별로 표 만들기)
 - AUC
 - f1 score (weighted, macro)
 - recall
 - precision

4. 문의 사항
 - normal 데이터의 이상치 패턴 (10월 10일)
 - 현재 연구의 문제점
 - 추가 설치된 wando01 access하는 방법, 그리고 이 데이터 정상인지 비정상인지

데드라인 : 논문 쓰는 일정에 맞춰서 알아서 하기.


각 알고리즘에 대한 스터디
- 히든 사이즈 : 높일수록 데이터를 정밀하게 세부적으로 학습하고 싶다. (16,32,64)
 - 정석 : 최적화
- 에포크 : 충분히 주면 됨.

- output_size : 0/1을 예측
"""